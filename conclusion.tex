\chapter{Conclusion}
When tackling the problem of Web Information Extraction in the context of the average user.
Having looked at some of them in Chapter 1, we have selected a subset of these issues:
Visual Feedback when performing annotations and creating Robust Wrappers for information 
extraction.

In our report, we have shown that a simple alignment algorithm can be applied iteratively to
produce the desired effect to give users a visual feedback of what they will be selecting.
We have also used machine learning to tackle the issue of creating robust wrappers in the form
of a decision tree. These wrappers are then used as backups when layouts change and the original
XPath fails to work. These solutions were then implemented in the grab\textit{smart} system.

There are still limitations with the current implementation, such as the poor performance of
the learnt model when it comes to non-header elements, or elements containing single worded
content. Also, the system captures information at the HTML tag level, but more useful information
may lie within the text content inside the tags.

\section{Contributions}
While grab\textit{smart} is still far from being a full-scale Web Information Extraction system,
it has the following advantages when compared to other Web IE solutions:
\begin{enumerate}
	\item It provides an immediate visual feedback when the user does the annotation process.
	\item From a usability perspective, we have shown that it is: Easy to install, relatively
	simple to use, and the extracted data can easily be viewed.
	\item It provides an alternative to the rigidness of the generated XPath with the decision
	tree wrapper alternative.
	\item The report also proposes a method for automating the detection layout changes given
	a chain of archived pages by using the Zhang-Shasha tree edit distance.
\end{enumerate}

\section{Future Work}
With the \textit{grabsmart} system only tackling a few of the problems in User-centric Web IE,
it leaves much room for future work to be done:

\subsection{User Interface}
The selection interface, or the bookmarklet, can be made to consider more
features of the selected elements. For example, the current implementation only takes into 
account the attribute values of the elements. Since the XPath syntax also allows for 
regular expression predicates, it is concievable for a method to be formulated that creates
a regex pattern common to all selected items. This would give a more ``content-based" XPath
that may be more selective.

The web application, being less of the focus in this project, could have more functionality
built in. Since the extracted data is already present on the database, one could create
ways in which this data could be manipulated and displayed. Currently, only the data from
the latest extraction is available. The data from the previous extractions could be used 
for comparisons with the current extracted data, examples of such data include Linux
distribution download rankings or book prices.

\subsection{Machine learning component}
For this project, we have made use of a library from the Apache project known as HtmlUnit.
This library provides us with a ``headless browser" from which to access the annotated pages,
but it has several drawbacks. Unlike a true browser, it is unable to parse the CSS associated
with the page, as well as the Javascript. This deprives us of many usefull visual features
that would be obvious to the user. An alternative to this would be the WebKit library, since
it is a widely used rendering engine. A headless instance of WebKit could function like a
browser, allowing us to query the the page for visual features that would be useful for 
classifying HTML elements.

\subsection{User-centric Web IE in General}
A good next step forward for the system would be to be able to do some form of focussed
crawling through the site for a ``chain" of pages that the user might want.

Since the focus of many scraping systems are lists of structured data, many of these are
paginated, and one of the limitations of our system is that the extractor does not go
beyond the pages the user has annotated. Being able to infer a navigation pattern that
goes through this paginated data, and then extracting this automatically would be useful
for users who do not know how to write a scraper.

