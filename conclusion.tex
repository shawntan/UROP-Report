 \chapter{Conclusion}
As of December 2010, there are 255 million websites on the internet, of which 21.4 million
were created in 2010\footnote{\url{http://mashable.com/2011/01/25/internet-size-infographic}}.
Our high level goal: to allow users to easily and simply extract any information from any 
of these websites. The sheer size of the web makes this task seem almost insurmountable.

While this primary goal has many challenges, in this report, we have chosen to address 
two challenges to pave the way towards a more user-centric web IE system. To provide users with a more accessible
user interface, grab\textit{smart} utilises a simple but effective alignment algorithm we call
PTA through an interactive and visual experience for its users to specify the information they want extracted
To create a more robust wrapper that well seamlessly work even after website layout redesign.

Overall, our user evaluation shows that the visual selection interface is doing well, 
while the machine learnt classifier performs comparably to another work in robust 
web IE. These are promising results and more can be done to improve upon them.

There are also other limitations with the current implementation, such as the poor performance of
the learnt model when it comes to non-header elements, or elements containing single worded
content. Also, the system captures information at the HTML tag level, but more useful information
may lie within the text content inside the tags.

\section{Contributions}
grab\textit{smart} it has the following advantages when compared to other Web IE solutions:
\begin{enumerate}
	\item It provides an immediate visual feedback when the user does the annotation process.
	\item It is: Easy to install, relatively simple to use.
	\item It provides an alternative to the rigidness of the generated XPath with the decision
	tree wrapper alternative.
	\item It employs our proposed method for automating the detection layout changes given
	a chain of archived pages using the Zhang-Shasha tree edit distance.
\end{enumerate}

\section{Future Work}
With the \textit{grabsmart} system only tackling a few of the problems in User-centric Web IE,
it leaves much room for future work to be done:

\subsection{User Interface}
The  bookmarklet, should consider more
features of the selected elements. For example, the current implementation only takes into 
account the attribute values of the elements. Since the XPath syntax also allows for 
regular expression predicates, it is concievable for a method to create
a regex pattern common to all selected items. This would give a more ``content-based" XPath
that may be more selective.

The web application, being less of the focus in this project, could have more functionality
built-in. Since the extracted data is already present on the database, one could create
ways in which this data could be manipulated and displayed. Currently, only the data from
the latest extraction is available. The data from the previous extractions could be used 
for comparisons with the current extracted data, examples of such data include Linux
distribution download rankings or book prices.

\subsection{Machine learning component}
For this project, we have made use of a library from the Apache project known as HtmlUnit.
This library provides us with a ``headless browser" from which to access the annotated pages,
but it has several drawbacks. Unlike a true browser, it is unable to parse the CSS associated
with the page, as well as the Javascript. This deprives us of many usefull visual features
that would be obvious to the user. An alternative to this would be the WebKit library, since
it is a widely used rendering engine. A headless instance of WebKit could function like a
browser, allowing us to query the the page for visual features that would be useful for 
classifying HTML elements.

\subsection{User-centric Web IE}
The next step forward for the system would be to do focussed
crawling through the site for a ``chain" of pages that the user might want.
Such a system is a viable research problem that could be looked into.

Since the focus of many scraping systems are lists of structured data, many of these are
paginated, and one of the limitations of our system is that the extractor does not go
beyond the pages the user has annotated. Being able to infer a navigation pattern that
goes through this paginated data, and then extracting this automatically would be useful
for users who do not know how to write a scraper.



