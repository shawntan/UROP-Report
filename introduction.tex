\chapter{Introduction}

\section{Motivation}
Web Information Extraction (IE) is extracting data information from the web into a structured form.
Since many web documents today are in HTML, and increasingly generated by server scripts and
template-based, web information extraction is slightly different from information extraction
from free text. In IE, procedures that extract selected information from structured or semi-structured
documents like HTML are known as \textit{wrappers}.

While many systems for web information extraction have been developed over the years, many of
these are used in corporate settings, and generally extract huge amounts of data from the web
into a form more suited for search and retrieval. While this may be within the reach of
corporations, a user who browses the web for leisure does not have access to such resources.
Another drawback of these systems is that the process of getting them up and running usually
involve many hours of labelling and training work. As a result, these systems are inaccessible
to users who do not have the time to do this.

One might argue that this is where RSS feeds come in, to provide users updates to constantly
changing data on their pages. However, the data provided on RSS is usually controlled by the
site in question, and may not provide the data that the user actually needs. One other
alternative to tackle this problem would be to write ``screen scrapers", as this would provide
complete control over the data extracted. This approach faces another set of problems, the most
glaring being that most users are not familiar with programming, let alone the many other
technical issues faced when doing screen scraping.


\begin{table}[t]
\centering
\singlespacing
\small
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
					&	RSS Feeds	&	Screen Scrapers \\
\hline
\hline
	Availability	&
	Lies with content provider &
	Anything that is displayed can be scraped \\
\hline
	Content Extracted &
	Lies with content provider &
	Lies with user \\
\hline
	Affected by Layout &
	Content provider provides content, no layout involved &
	Breaks on layout change \\
\hline
	Format &
	Still requires parsing of RSS XML in order to maipulate data. &
	Can extract into any format \\
\hline
	Technical Knowledge required &
	User has only to know how to use RSS. &
	Requires programming experience \\
	\hline
\end{tabular}
\caption{Pros \& Cons of using RSS or wrappers (screen-scrapers)}
\label{tab:template}
\end{table}
\input userstudy.tex

As such, we see a gap here which needs to be filled by bringing web IE closer to the average
user by making creation of wrappers more user friendly, and at the same time, creating wrappers
that are robust and resistant to layout changes. In essence, a ``super scraper" for
user-centric web information extraction.

\section{Goals \& Challenges}
The overall goal of user-centric web information extraction would be to offer a way for users
to be able to scrape data as simply as possible. Some of the issues that would be faced include
overcoming the user's lack of knowledge in programming, but still being able to provide a method
for the user to be able to scrape relevant data from a website.

In this report, we present a system that attempts to solve some of these problems by doing the
following:
	\begin{enumerate}
		\item Provide an intuitive interface for labelling that is platform agnostic, and takes
		 advantage of the many advancements in Javascript. This will provide the user with an
		 immediate visual feedback as to the items that he/she will be extracting, and at the
		 same time reduce the amount of labelling that needs to be done.
		\item Create a more robust framework for extraction of the selected information using
		machine learning. The classifier will have less focus on the HTML structural
		information of the tags in order to be resistant to any layout changes made to the page.
	\end{enumerate}

While this does not solve all the issues with user-centric web IE, we hope that these contributions
would take web IE a step closer to the average user.

We will describe our approach in the following chapters. In Chapter \ref{chap:relatedwork} we
will look at some of the other work that has been done in the areas of visual annotation of
training data, XPath generation and creation of robust wrappers. In Chapter \ref{chap:method},
we will describe the various algorithms and methodologies involved in the creation of the
system. In Chapter \ref{chap:implementation}, we will look at the implementation of the system,
while finally in Chapter \ref{chap:evaluation} we will see the 2 main components of the system
being evaluated on 2 fronts, a human evaluation for the selection interface, and an evaluation
of the machine learning component.