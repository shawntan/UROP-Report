\chapter{Introduction}

\section{Motivation}
Web Information Extraction (Web IE) is extracting data from the web,
which are commonly unstructured or semi-structured into a structured form.
Since many web documents today are increasingly template-based, generated by server scripts,
 web IE is slightly different from IE
from free text. This allows for more accurate methods for IE, and systems have been developed
to extract the differing information from the repetitive raw data.
In IE, procedures that extract selected information from structured or semi-structured
documents like HTML are known as \textit{wrappers}.

While many systems for web IE have been developed over the years, many of
these are used in corporate settings, and are commonly used to extract huge amounts of data from the web
into a form to aid search and retrieval. This is within the reach of
corporations, but users who browse the web for leisure do not have access to such resources.
Another drawback of these systems is that the process of getting them up and running usually
involves many hours of labelling and training work. As a result, these systems are inaccessible
to users who do not have the time to do this. 

One might argue that this is where RSS feeds come in, to provide users updates to constantly
changing data on their favourite web pages. However, the data provided through RSS is usually controlled by the
corresponding sites, and may not provide the exact information that the user actually wants. An
alternative for users is for them to write their own ``screen scrapers". This would provide
complete control over the data extracted from the page of their liking, but, however,
most users are unfamiliar with programming, let alone the many
technical issues involved in screen scraping.


\begin{table}[t]
\centering
\singlespacing
\small
\begin{tabular}{|p{3cm}|p{5cm}|p{5cm}|}
\hline
					&	RSS Feeds	&	Screen Scrapers \\
\hline
\hline
	Availability	&
	Lies with content provider &
	Anything that is displayed can be scraped \\
\hline
	Content Extracted &
	Lies with content provider &
	Lies with user \\
\hline
	Affected by Layout &
	Content provider provides content, no layout involved &
	Breaks on layout change \\
\hline
	Format &
	Still requires parsing of RSS XML in order to maipulate data. &
	Can extract into any format \\
\hline
	Technical Knowledge required &
	User has only to know how to use RSS. &
	Requires programming experience \\
	\hline
\end{tabular}
\caption{Pros \& Cons of using RSS or wrappers (screen-scrapers)}
\label{tab:procons}
\end{table}


Considering various aspects of RSS feeds and screen scrapers (see Table \ref{tab:procons}),
we see an opportunity to make web IE more accessible for 
users by making it more user friendly,without the need for any programming knowledge.
More than that, we also want the created wrappers to be robust and resistant to website layout
changes with minimal user intervention.
In essence, a ``super scraper" for user-centric web information extraction.

\section{Goals \& Challenges}
Our primary goal of user-centric web IE is to offer a simple and easy way for users
to be able to extract data from the web. 

We have identified two challenges for this goal that we will address in this thesis.
Firstly, the system must allow users to specify the information
they want extracted without requiring them to know how to program.
Secondly, the system must work seamlessly even at the presence of a web site layout
change.

To address these challenges, we propose
grab\textit{smart}, a system with the following features:
	\begin{enumerate}
		\item Provide an intuitive and visual interface for labelling that is platform agnostic.
		grab\textit{smart}'s user interface for specifying information on a web page to extract
		 provides the users
		 immediate visual feedback as to items that will be extracted. This makes the labelling process more interactive, and at the
		 same time reduces the amount of labelling that needs to be done.
		\item Provide a robust framework for extraction of the selected information using
		machine learning. Users would have their wrappers still work after layout changes, with minimal
		user intervention.
	\end{enumerate}

The remaining parts of the thesis is organised as follows. In Chapter \ref{chap:relatedwork} we
will look at some of the other work that has been done in the areas of visual annotation of
training data, XPath generation and creation of robust wrappers. In Chapter \ref{chap:method},
we will describe the various algorithms and methodologies involved in the creation of the
system. In Chapter \ref{chap:implementation}, we will look at the implementation of the system,
while finally in Chapter \ref{chap:evaluation} we will see the 2 main components of the system
being evaluated on 2 fronts, a human evaluation for the selection interface, and an evaluation
of the machine learning component.
