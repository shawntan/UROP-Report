\section{Method}
\label{chap:method}
In the following sections, we will elaborate on the different techniques used in this report.
In the first section we will describe traversal path alignment algorithm, which we
propose to overcome the issues of current methods of XPath generation by providing
a way to create viable XPaths based on elements that users have selected or rejected,
giving them a visual understanding of what will be extracted as they complete the process.

Next, we address the approach we take in trying to create a robust wrapper for the site.
We propose a framework for learning models for classification of HTML tags or DOM nodes by
learning from extracted instances using the XPath.

\subsection{Path Traversal Alignment (PTA)}
A tag's traversal path from the \url{<BODY>} tag is an important aspect which describes tags which are visually similar.
Parents of selected tags with similar tag names or that have similar
attributes are likely to be tags that look visually similar on the page. This also implies that
 the elements are probably items that that the user would also be looking for.

We propose using a modification of the Needleman-Wunsch \cite{Needleman1970} global alignment algorithm for this
purpose. Each traversal path could be seen as a sequence of symbols that need to be aligned
based on their common attributes. The more attribute-values they have in common, the more
likely they are ancestors of elements of the same type. However, when reconstructing the
aligned sequence, one needs to be aware that the symbols are not entirely equivalent, and that
only the common attributes should be retained.

Hence, we make the following modifications to the Needleman-Wunsch algorithm:
\begin{enumerate}
	\item The scoring function used is based on the number of attributes in common
	between the two elements.
	\item The method for reconstructing the aligned path is given as a parameter. We will
	describe in the following sections why this is needed.
\end{enumerate}

Given the input of 2 arrays $P$ and $Q$ of elements, the scoring function is given as
$\textsc{Score}(P_i,Q_j) = |P_i \cap Q_j|$, where $P_i$ and $Q_j$ are elements (sets of
attributes) along $P$ and $Q$. After the alignment, a new generalised traversal path is
created, $P'$, such that $P'_k = P_i \cap Q_j$, where $P_i$ and $Q_j$ are elements in the
original traversal paths that were aligned.


When selecting additional elements for generalising the XPath, the $\lambda$ function would be
defined as $\lambda(P_i,Q_j) = P_i \cap Q_j$. This would reconstuct a generalised traversal
path with common elements and their common attributes. Gaps in the path are represented as ``*"
 in the output. During the serialisation of the XPath, groups of these are converted to
 \url{//} to represent any descendant.

This process is a bottom-up approach: Users start with a single element, and as more items are
selected, the items captured by the XPath generated grows. For each element along the selected
item's path, the similarities between the tag's attributes are kept, and the differences
removed. This results in an extraction rule with fewer and fewer constraints as more elements
are selected. Using alignment of the path elements, XPath rules can be generalised for elements at different
levels of the DOM tree. This XPath will include both
selected elements, as well as other anchor tags that the user may be interested in.


It is important to note that serialising the generalised model to its XPath form would result
in loss of information. As such the model in its unserialised form is kept, and used again when
another item on the page is selected. Since this process has an associative property, the order
in which items are selected on the page does not matter, and the user can then select three items
or more in order to specify the items he wants.

Once the user has defined a region for extraction, he/she may find that the region includes
items that are irrelevant. Our objective was to make this process part of the selection process as
well. To achieve this, we used the same alignment algorithm, with $P$ as the already
generalised path, and $Q$ as the new traversal path of the element that the user wishes to
reject, with $\lambda$ defined as $\lambda(P_i,Q_j) = Q_j - (P_i \cap Q_j)$.

There are some limitations to this method. When $Q_j = P_i \cap Q_j$ for all $i,j$
in the aligned path, that rejection path cannot be used. This usually implies that the
traversal paths of the wanted elements and the rejected elements are indistinguishable using
the considered attributes. To address this limitation, the user is not allowed to select elements that
cause such an event. Once the first rejection element has been generalised with the PTA
algorithm, additional rejection elements can then be generalised using the first algorithm.

Also, the mentioned method of generalising the XPath is a form of a multiple sequence alignment,
but since our method is approached using a sequence of pairwise alignments, the optimal alignment may
not be achieved. However, in our evaluation the selection scopes of the generated XPaths are found by
users to be usable for scraping.

\subsection{Robust Extraction}
\label{section:extraction}
The PTA algorithm enables us to generate a fixed rule (XPath) to extract data from the given pages.
However, XPath is heavily reliant on the structure of the DOM page in its present form: Any element
extracted is defined by its ancestor nodes and their attributes. Once this is changed, which is
likely due to a layout change, the same XPath may no longer be applicable to the current page. 

To extract data even after a change in layout, a method of extraction
without the use of XPath is required. Our approach for this system uses a machine learned model.
The rest of the chapter elaborates on the details of the implementation.

In our framework, we use the provided XPaths together with the URLs of the pages to be extracted
as annotated documents. Each node in the DOM is treated as a separate instance, and labelled
based on the XPath it was extracted by. The XPaths are assumed to be mutually exclusive. We have
chosen to use decision trees because the generated models are small, and can be easily interpreted.
The generation of these models are also relatively faster than other classification models. Since
CPU time and memory space are constrained resources, decision trees were suitable for our needs.

	To keep the model more robust and resistant to layout changes on the original site,
content features are extracted in order to use less of the HTML structure for extraction.
In this system, a J48 decision tree classifier is used. Positive examples of HTML
elements are those returned when the XPath is applied to the given page.
The following features are extracted:
Word occurrence count, or tokens (Numeric),
Tag name (Discrete),
Previous sibling, next sibling and parent tag name (Discrete),
Number of words/tokens (Numeric),
Ending with character (:,-,.) (Discrete),
and Header (Discrete)
	

	
	%need to run more tests on SVMs and dec trees
	
	For subsequent extractions, the server will first use the available XPath for extraction from the site.
	If the XPath rule fails to find elements for extraction, the learnt classification model is used.

	We crawled the site for other pages which had elements captured
	by the same set of XPaths in order to increase the training instances we can
	use for the machine learner. This gave us more instances to train the
	classifier on, and gave better results. Note that this is also done automatically
	without user intervention.
	
The crawler starts with a given set of pages, and crawls links with URLs similar to the given pages. The similarity score is given by the edit distance between the link URL and the URL of the wanted pages. A heap is used to store the seen links, and the most similar URL selected at every round. The crawler stops when it has collected a fixed number of instances, which is now set at 500.
	

It should be noted that, from our tests, the learnt model does better on heading-type results, while other
fields tend to have a low precision score, we will discuss the evaluation results in detail
in Section 4.
	