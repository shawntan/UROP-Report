\section{The grab\textit{smart} System}
\label{section:implementation}
This chapter details the implementation of the methods mentioned before into a working system.
The system consists of three main components:
\begin{enumerate}
	\item \textbf{Selection Interface}
	This is implemented as a bookmarklet which the user can simply drag into the toolbar of the browser.
	\item \textbf{Web Application}
	This is the frontend of the system, allowing the user to create, update and delete their extractors,
	and also to be able to view their extracted data.
	\item \textbf{Machine Learning Component}
	This is the component that creates a classification model based on the generated XPath and the pages selected to be extracted from.
\end{enumerate}

We intend to introduce this system with the following scenario: John wants to be informed of 
the latest books about information extraction on bookdelivery.co.uk. In the subsequent sections
we will describe John's workflow, and how the methods described in Chapter \ref{chap:method} 
fit into the overall system.

\section{Selection Interface}
The selection interface described in this section aims to provide visual feedback to John
when building a suitable wrapper for the page chosen by John. The interface is implemented
in the form of a bookmarklet which John simply has to drag and drop into his browser
toolbar. This bookmarklet can then be activated when the he arrives at a page which he/she wants
to have something extracted. Clicking on items will select them, and subsequent clicking
will expand the scope of the extraction, with the items to be extracted highlighted.


When the user clicks on a new element on the page,
the XPath is recalculated using the PTA algorithm, and then used to highlight the captured items on the page.
Since the generalisation of the XPath is an iterative process, the user is able to perform
actions step by step, and see how their actions affect the result.
This way, the user understands the changes he/she has made to the extraction scope as he/she
clicks on additional items. The user is then able to perform this task for any number of labels
the user thinks is appropriate for the extractor he/she is creating.

In this case, John navigates to the search results for ``Information Extraction"
on bookdepository.co.uk, and activates the bookmarklet. He then selects the first
two book titles and the selection interface automatically highlights all the other
titles on the page in yellow. This means that the items will be extracted once the
extractor is activated. He then saves this label, then does the same for the prices.


\subsection{Web Application}

In order to extract data from bookdepository.co.uk, John would have to create a new 
\textbf{extractor}. This can be done through the bookmarklet when he is at the page 
he wishes to capture. Once he starts selecting items to capture at a given page,
he would have created a new \textbf{page} that the extractor is required to extract.
Once he is done with a capture region, this would be saved under the extractor as a
new \textbf{label}. 

The web application we have developed aims to provide the user a way to modify and
 manipulate the concepts we have mentioned.

\textbf{Dashboard} This is the first page that John sees
 after he has logged in. The page lists the various extractors that he has created, and 
 he is able to delete them or view the extractors in greater detail by clicking ``View".

\textbf{Extractor view} The detailed view of the extractor shows several items:
 List of the labels, together with their corresponding XPath,
 the list of pages that the labels will be applied on and
the latest list of data extracted from the data, if it exists.

 John is, again, able to delete any labels or pages that he no longer wishes to extract
 using the extractor. Also, the page has a linked RSS feed that the user can subscribe to.
 
 It is at this page that the John is able to do a test run of the extractor. Once satisfactory,
 he can then set a daily time for the data to be extracted.
 
 Once John has arrived at this point, his extractor is now in place to provide him updates
 via RSS daily.
\section{Machine Learning Component}
After John clicks on ``Extract now" or when the extractor is scheduled to be executed, the
machine learning component will retrieve the relevant pages and extract the data using the
extractor's XPath. Using this set of data, it will then proceed to retrieve more similar pages
with the method described in the previous chapter.

This process is meant to be transparent to the user, so the user should not have to make
any changes to his configured extractor. The extractor should then work even though a layout 
change has occurred.

	The extracted data is inserted back into the database, and then made available to the user.
At the same time, features are extracted from this data in order to create a model for
extraction for future use.  For John, the process takes place completely in the background.
When the layout of bookdepository.co.uk changes, the same data is still expected to be extracted.
This is in line with what we expect from a user-centric web IE system.

	Now that we have seen how the various methods described in Chapter \ref{chap:method} 
fit into the rest of the system, we will look at how well the system performs in 
Chapter \ref{chap:evaluation}.
